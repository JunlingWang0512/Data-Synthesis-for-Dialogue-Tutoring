from transformers import AutoTokenizer, AutoModel, GPT2LMHeadModel, GPT2Tokenizer
from scipy.spatial.distance import cosine
import textstat
import json
import pandas as pd
import openai
import re
import csv
import random
import nltk
from nltk.util import ngrams
from nltk.probability import FreqDist
from scipy.stats import entropy
from collections import Counter

def calculate_bigram_entropy(dialog):
    # Tokenize sentences and generate bigrams
    bigrams = []
    for sentence in dialog:
        tokens = nltk.word_tokenize(sentence)
        bigrams.extend(list(ngrams(tokens, 2, pad_left=True, pad_right=True)))

    # Calculate frequencies of bigrams
    freq_dist = FreqDist(bigrams)

    # Calculate probabilities
    probabilities = [freq_dist[bigram] / len(bigrams) for bigram in freq_dist]

    # Compute entropy
    bigram_entropy = entropy(probabilities, base=2)

    return bigram_entropy

openai.api_key = "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"



def extract_score(text):
    match = re.search('Score:\s*(\d+(?:\.\d+)?)', text)
    if match:
        return float(match.group(1))
    else:
        return None
def generate_response0(prompt,model):
    #intruduction of the task
    # messeage_head = "Task: Generate a high-quality student question that corresponds to the teacher response. I will provide you with a sentence as the teacher's response, which is the answer to the student's question.\n\nInstructions:\n\n1 Your generated student question should be related to the content of the sentence and demonstrate an understanding of the concept or topic discussed in the sentence.\n2 The teacher's response should be unchanged, just put it into json\n3 Use the following JSON format for the dialogue:\n{\"dialogues\": [{\"speaker\": \"student\",\"text\": \"\"},{ \"speaker\": \"teacher\",\"text\": \"\"}]}\n4 When generating a student question, try to put yourself in the shoes of someone who is genuinely curious and wants to learn more about the topic.\n5 Be creative and try to generate a question that a curious student might ask in a real classroom setting.\n6 Only output one JSON variable, do not output anything else!\n7 the teacher's response is: "
    #define the format of the output
    # messeage_end = '\n\nYou should generate one question for one sentence, and the sentence should be the answer for your question. Your question should be high quality and like human. Generate question in the following JSON format, the sentence should be in teacher place, while your question should be in student place:{  "dialogues": [  {  "speaker": "student",  "text": ""  },  {  "speaker": "teacher",  "text": ""  },...  ]}'
    #combine the introduction and the prompt into the input
    messeage_content = prompt

    # print(messeage_content)
    completion = openai.ChatCompletion.create(
        # model="gpt-3.5-turbo", 
        model=model,
        messages=[{"role": "user", "content": messeage_content}]
    )

    return completion
def calculate_coherence_score(dialog):
    
    dialog_str = "\n".join([f"Teacher asks: {dialog[i]}\nStudent answers: {dialog[i+1]}" for i in range(0, len(dialog), 2)])
    
    prompt = f"Assuming you are a linguistic expert, your task is to assess the coherence of the following dialogue generated by an AI system. This dialogue is structured as a teacher-student interaction centered around teaching a passage from a textbook, only the teacher part is generated by AI. When we say 'coherence', we're asking: Does the dialogue logically flow and stay consistent with the conversation's context? Does the AI-generated dialogue maintain the thematic context from beginning to end? Please provide a coherence score from 0 to 10, where 0 signifies no coherence and 10 denotes excellent coherence. The dialog is: \n\n{dialog_str}\n\n Please provide me with the score only in your response in this format: Score:<number>"
    # print('prompt:',prompt)
    # print('__________________________generated______________________________________________')
    # inputs = tokenizer.encode(prompt, return_tensors='pt', truncation=True, max_length=512)
    # outputs = model.generate(inputs, max_length=512, do_sample=True)
    result = generate_response0(prompt,'gpt-4-0314')
    input_string = result['choices'][0]['message']['content']
    
    return extract_score(input_string)

def calculate_correctness_score(dialog):
    
    dialog_str = "\n".join([f"Teacher asks: {dialog[i]}\nStudent answers: {dialog[i+1]}" for i in range(0, len(dialog), 2)])
    
    prompt = f"Assuming you are a linguistic expert, your task is to assess the correctness of the following dialogue generated by an AI system. This dialogue is structured as a teacher-student interaction centered around teaching a passage from a textbook, only the teacher part is generated by AI. When we say 'correctness', we're asking: Does the dialogue provide accurate and factual question for each response? \n\n Please provide a coherence score from 0 to 10, where 0 signifies all questions are not correct and 10 denotes excellent correctness. The dialog is: \n\n{dialog_str}\n\n Please provide me with the score only in your response in this format: Score:<number>"
    # print('prompt:',prompt)
    # print('__________________________generated______________________________________________')
    # inputs = tokenizer.encode(prompt, return_tensors='pt', truncation=True, max_length=512)
    # outputs = model.generate(inputs, max_length=512, do_sample=True)
    result = generate_response0(prompt,'gpt-4-0314')
    input_string = result['choices'][0]['message']['content']
    extract_score(input_string)
    return extract_score(input_string)
# import language_tool_python

# Initialize models and tokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
model = AutoModel.from_pretrained('bert-base-uncased')

# Initialize language tool
# tool = language_tool_python.LanguageTool('en-US')

def get_vector(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True)
    outputs = model(**inputs)
    return outputs.pooler_output[0].detach().numpy()


def calculate_similarity(text1, text2):
    vec1 = get_vector(text1)
    vec2 = get_vector(text2)
    return 1 - cosine(vec1, vec2)



# Evaluate Dialogue
# dialogue = [
#     "Hello, how are you?",
#     "I'm fine, thank you. How about you?",
#     "I'm good too, thanks for asking."
# ]

# Check Coherence


# Check Grammar
# grammar_errors = []
# for sentence in dialogue:
#     grammar_errors.append(check_grammar(sentence))

# print("Total Grammar Errors: ", sum(grammar_errors))

# Assuming the first statement was the AI question and the second the text, check relevance
def flesch_reading_ease(dialogue): #fluent score
    # Concatenating all sentences in the dialogue into a single text
    dialogue_text = " ".join(dialogue)
    # Calculate the Flesch Reading Ease score
    flesch_reading_ease = textstat.flesch_reading_ease(dialogue_text)
    # Scale to 0-10, where 10 is the easiest text to read
    scaled_score = flesch_reading_ease / 10
    # Clamp the score to the maximum of 10
    scaled_score = min(10, scaled_score)
    return scaled_score



def relevance_score(dialogue):
    local_relevance_scores = []
    for i in range(len(dialogue)-1):
        local_relevance_scores.append(calculate_similarity(dialogue[i], dialogue[i+1]))
    scaled_score = (sum(local_relevance_scores)/len(local_relevance_scores)) * 10
    return scaled_score



file_path = '/cluster/scratch/wangjun/dialogue_inpainting5_18_flan_lora_xl/work/ukp/huggingface/6_27_algebra_search/HuggingfaceSearchJob.8FtuwoC7t0JX/output/search_output_post.json'

data = []
with open(file_path, 'r') as f:
    decoder = json.JSONDecoder()
    text = f.read()
    while text:
        obj, idx = decoder.raw_decode(text)
        data.append(obj)
        text = text[idx:].lstrip()

# Initialize a list to store the dialog texts and their corresponding scores
score_data = []

# randomly select 20 dialogs for evaluation
# random.seed(123)
# selected_data = random.sample(data, 100)
selected_data = data
for obj in selected_data:
    dialog = obj['utterances']
    print(dialog)
    if dialog == []:continue
    score_dict = {}
    score_dict['dialog'] = dialog
    score_dict['relevance'] = relevance_score(dialog)
    score_dict['flesch_reading_ease'] = flesch_reading_ease(dialog)
    score_dict['bigram_entropy'] = calculate_bigram_entropy(dialog)
    # score_dict['correctness'] = calculate_correctness_score(dialog)
    # uptake
    # score_dict['coherence'] = calculate_coherence_score(dialog)
    
    # Append the dictionary to the score_data list
    score_data.append(score_dict)

# Convert the list of dictionaries to a DataFrame
df = pd.DataFrame(score_data)

# Calculate the average scores and append them to the DataFrame
avg_scores = df[['relevance', 'flesch_reading_ease', 'bigram_entropy']].mean()
df.loc['Average'] = ['Average'] + avg_scores.tolist()

print(df)
df.to_csv('algebra_post_test_metrics3_all.csv')

# file_path = '/cluster/scratch/wangjun/dialogue_inpainting5_18_flan_lora_xl/work/ukp/huggingface/6_27_business_search/HuggingfaceSearchJob.8FtuwoC7t0JX/output/search_output.json'
# decoder = json.JSONDecoder()
# data = []
# with open(file_path, 'r') as f:
#     text = f.read()
#     while text:
#         obj, idx = decoder.raw_decode(text)
#         data.append(obj)
#         text = text[idx:].lstrip()

# relevance_scores = []
# fluent_scores = []
# correctness_scores = []
# coherence_scores = []
# count = 0

# # randomly select 20 dialogs for evaluation
# selected_data = random.sample(data, 20)
# for obj in selected_data:
#     count += 1
#     print(count)
#     dialog = obj['utterances']
#     temp_relevance_score = relevance_score(dialog)
#     temp_fluent_score = flesch_reading_ease(dialog)
    

#     temp_correctness_score = calculate_correctness_score(dialog)
#     temp_coherence_score = calculate_coherence_score(dialog)

#     relevance_scores.append(temp_relevance_score)
#     fluent_scores.append(temp_fluent_score)
#     correctness_scores.append(temp_correctness_score)
#     coherence_scores.append(temp_coherence_score)
#     print(f'Dialog #{count}')
#     print('temp_relevance_score',temp_relevance_score)
#     print('temp_fluent_score',temp_fluent_score)
#     print('temp_correctness_score',temp_correctness_score)
#     print('temp_coherence_score',temp_coherence_score)

# avg_relevance_scores = sum(relevance_scores) / len(relevance_scores)
# avg_fluent_scores = sum(fluent_scores) / len(fluent_scores)
# avg_correctness_scores = sum(correctness_scores) / len(correctness_scores)
# avg_coherence_scores = sum(coherence_scores) / len(coherence_scores)

# df = pd.DataFrame({
#     'relevance': [avg_relevance_scores],
#     'fluent':[avg_fluent_scores],
#     'correctness':[avg_correctness_scores],
#     'coherence':[avg_coherence_scores]
# })

# print('the average result is:', df)
  
# algebra_post_output: relevance:   8.855082 fluent: 6.976059
# algebra_output:relevance:  8.800209 fluent: 6.955946

# business:search_output: 
# relevance  fluent  correctness  coherence
# 8.90022  4.9657        9.175       8.85