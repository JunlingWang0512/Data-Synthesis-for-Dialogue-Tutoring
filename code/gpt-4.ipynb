{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import re\n",
    "import csv\n",
    "\n",
    "openai.api_key = \"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\"\n",
    "\n",
    "def generate_response0(prompt,model):\n",
    "    #intruduction of the task\n",
    "    # messeage_head = \"Task: Generate a high-quality student question that corresponds to the teacher response. I will provide you with a sentence as the teacher's response, which is the answer to the student's question.\\n\\nInstructions:\\n\\n1 Your generated student question should be related to the content of the sentence and demonstrate an understanding of the concept or topic discussed in the sentence.\\n2 The teacher's response should be unchanged, just put it into json\\n3 Use the following JSON format for the dialogue:\\n{\\\"dialogues\\\": [{\\\"speaker\\\": \\\"student\\\",\\\"text\\\": \\\"\\\"},{ \\\"speaker\\\": \\\"teacher\\\",\\\"text\\\": \\\"\\\"}]}\\n4 When generating a student question, try to put yourself in the shoes of someone who is genuinely curious and wants to learn more about the topic.\\n5 Be creative and try to generate a question that a curious student might ask in a real classroom setting.\\n6 Only output one JSON variable, do not output anything else!\\n7 the teacher's response is: \"\n",
    "    #define the format of the output\n",
    "    # messeage_end = '\\n\\nYou should generate one question for one sentence, and the sentence should be the answer for your question. Your question should be high quality and like human. Generate question in the following JSON format, the sentence should be in teacher place, while your question should be in student place:{  \"dialogues\": [  {  \"speaker\": \"student\",  \"text\": \"\"  },  {  \"speaker\": \"teacher\",  \"text\": \"\"  },...  ]}'\n",
    "    #combine the introduction and the prompt into the input\n",
    "    messeage_content = prompt\n",
    "\n",
    "    print(messeage_content)\n",
    "    completion = openai.ChatCompletion.create(\n",
    "        # model=\"gpt-3.5-turbo\", \n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": messeage_content}]\n",
    "    )\n",
    "\n",
    "    return completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2= \"Task: write a paragraph of literature review of the topic: information-seeking dialogues for my thesis. \\n\\n Instructions:\\n 1 please use author names instead of paper names in the literature review \\n 2 You should state the contributions and limitations of each paper \\n 3 use the following papers and corresponding summary and limitations \\n paper 1. Structuring collaborative information-seeking dialogues \\n Summary: Wrote by Adelheit Stein and Elisabeth Maier: This paper presents a conversational interaction model that can be applied to graphical and multimodal interactions. The model comprises two interrelated parts: first, the description of local discourse structures and functional interrelations between dialogue acts to capture (local) conversational tactics, and second, the description of global structures by so-called dialogue scripts which are related to (global) information-seeking strategies.\\n Limitation: Limited applicability. The paper presents a specific framework for structuring collaborative information-seeking dialogues that may not be applicable to all scenarios or contexts.\\n paper 2. A Constraint-Based Approach for Cooperative Information-Seeking Dialogue\\n Summary: Wrote by Yan Qu and Nancy Green: This paper presents a dialogue generation model that uses a Constraint-Based Problem-Solver (CBPS) to support cooperative mixed-initiative information-seeking dialogue. Use of the CBPS enables a dialogue system to 1) incrementally interleave query construction with solution construction 2) immediately detect under-constrained and overconstrained information requests, and 3) provide cooperative responses when these types of problems are detected\\n Limitations: Limited scalability. The approach is based on a set of pre-defined constraints, which may not scale well to large or complex information-seeking tasks.\\n paper 3. Modeling the illocutionary aspects of information-seeking dialogues\\n Summary: This paper presents a dialogue model that incorporates the illocutionary aspects of information-seeking dialogues. Directive, commissive, and assertive types of dialogue acts (e.g., asking, offering, rejecting, answering, and evaluating) are represented by a complex transition network. The model determines all legitimate types and sequences of dialogue acts and regulates role assignments (e.g., when the information seeker and the information provider temporarily exchange their roles). Finally, an approach to integrating the illocutionary layer with other layers—which deal with thematical and rhetorical coherence—is outlined\\n Limitations: Limited scope. The paper focuses only on modeling illocutionary acts in information-seeking dialogues, without addressing other important aspects such as task coordination and feedback.\\n paper 4. QRFA: A Data-Driven Model of Information-Seeking Dialogues\\n Summary: This paper presents a new model for information-seeking dialogues called QRFA (Query, Request, Feedback, Answer). The authors apply process mining techniques to discover patterns in conversational transcripts across multiple conversational datasets from different domains. They show that their QRFA model better reflects conversation flows observed in real information-seeking conversations than models proposed previously. Moreover, QRFA allows them to identify malfunctioning in dialogue system transcripts as deviations from the expected conversation flow described by their model via conformance analysis\\n Limitations: Dependency on annotated data. The proposed model relies on annotated data for training and evaluation, which may be difficult and expensive to obtain for certain types of information-seeking dialogues.\\n paper 5. Introducing MANtIS: a novel Multi-Domain Information Seeking Dialogues Dataset\\n Summary: Wrote by Gustavo Penha et al.: This paper presents a novel conceptual model for conversational search defined in terms of conversational goals. The authors elicit how existing tasks and test collections from the fields of information retrieval (IR), natural language processing (NLP), and dialogue systems (DS) fit into this model. They describe a set of characteristics that an ideal conversational search dataset should have. Lastly, they introduce MANtIS, a large-scale dataset containing multi-domain and grounded information-seeking dialogues that fulfill all their dataset desiderata. They provide baseline results for conversation response ranking and user intent prediction tasks\\n Limitations: Limited dialogue complexity. The dataset contains only two-turn dialogues, which may not capture the complexity of real-world information-seeking dialogues with multiple turns.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"what is the method pipeline of this paper: Open-Domain Question Answering Goes Conversational via Question Rewriting?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Task: write a program to trade between space id and usdt in binance:\\ninstructions: \\n1. detect the trend of price of spaceid to usdt every 5 seconds, if the price is keep rising, i will buy 23 space id in the market price, if my price of loss is higher than %2, I will get all space id transfer to usdt\\n2. using a moving average to assess the trend over a longer time frame. For example, you could use a short-term moving average (e.g., 10 periods) and a long-term moving average (e.g., 20 periods). This would help to filter out random price fluctuations and provide a clearer indication of the overall trend\\n2. Incorporate additional technical indicators to supplement the moving averages and improve the accuracy of your strategy. For example, you could use the RSI (Relative Strength Index) or MACD (Moving Average Convergence Divergence) to confirm the trend's direction before initiating a trade.\\n3. Consider using a trailing stop loss instead of a fixed percentage stop loss. This would allow you to lock in profits as the price of SPACEID increases while still offering protection in case of a sudden downturn in the market\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: write a program to trade between space id and usdt in binance:\n",
      "instructions: \n",
      "1. detect the trend of price of spaceid to usdt every 5 seconds, if the price is keep rising, i will buy 23 space id in the market price, if my price of loss is higher than %2, I will get all space id transfer to usdt\n",
      "2. using a moving average to assess the trend over a longer time frame. For example, you could use a short-term moving average (e.g., 10 periods) and a long-term moving average (e.g., 20 periods). This would help to filter out random price fluctuations and provide a clearer indication of the overall trend\n",
      "2. Incorporate additional technical indicators to supplement the moving averages and improve the accuracy of your strategy. For example, you could use the RSI (Relative Strength Index) or MACD (Moving Average Convergence Divergence) to confirm the trend's direction before initiating a trade.\n",
      "3. Consider using a trailing stop loss instead of a fixed percentage stop loss. This would allow you to lock in profits as the price of SPACEID increases while still offering protection in case of a sudden downturn in the market\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Task: write a literature review of information-seeking dialogues for my thesis. \\n\\n Instructions:\\n 1 please use author names instead of paper names. and you should state the limitations of each p I provide the following papers for your consideration.\\n1. Structuring collaborative information-seeking dialogues \\n Summary: Wrote by Adelheit Stein and Elisabeth Maier: This paper presents a conversational interaction model that can be applied to graphical and multimodal interactions. The model comprises two interrelated parts: first, the description of local discourse structures and functional interrelations between dialogue acts to capture (local) conversational tactics, and second, the description of global structures by so-called dialogue scripts which are related to (global) information-seeking strategies.\\n Limitation: Limited applicability. The paper presents a specific framework for structuring collaborative information-seeking dialogues that may not be applicable to all scenarios or contexts.\\n 2. A Constraint-Based Approach for Cooperative Information-Seeking Dialogue\\n Summary: Wrote by Yan Qu and Nancy Green: This paper presents a dialogue generation model that uses a Constraint-Based Problem-Solver (CBPS) to support cooperative mixed-initiative information-seeking dialogue. Use of the CBPS enables a dialogue system to 1) incrementally interleave query construction with solution construction 2) immediately detect under-constrained and overconstrained information requests, and 3) provide cooperative responses when these types of problems are detected\\n Limitations: Limited scalability. The approach is based on a set of pre-defined constraints, which may not scale well to large or complex information-seeking tasks.\\n 3. Modeling the illocutionary aspects of information-seeking dialogues\\n Summary: This paper presents a dialogue model that incorporates the illocutionary aspects of information-seeking dialogues. Directive, commissive, and assertive types of dialogue acts (e.g., asking, offering, rejecting, answering, and evaluating) are represented by a complex transition network. The model determines all legitimate types and sequences of dialogue acts and regulates role assignments (e.g., when the information seeker and the information provider temporarily exchange their roles). Finally, an approach to integrating the illocutionary layer with other layers—which deal with thematical and rhetorical coherence—is outlined\\n Limitations: Limited scope. The paper focuses only on modeling illocutionary acts in information-seeking dialogues, without addressing other important aspects such as task coordination and feedback.\\n 4. QRFA: A Data-Driven Model of Information-Seeking Dialogues\\n Summary: This paper presents a new model for information-seeking dialogues called QRFA (Query, Request, Feedback, Answer). The authors apply process mining techniques to discover patterns in conversational transcripts across multiple conversational datasets from different domains. They show that their QRFA model better reflects conversation flows observed in real information-seeking conversations than models proposed previously. Moreover, QRFA allows them to identify malfunctioning in dialogue system transcripts as deviations from the expected conversation flow described by their model via conformance analysis\\n Limitations: Dependency on annotated data. The proposed model relies on annotated data for training and evaluation, which may be difficult and expensive to obtain for certain types of information-seeking dialogues.\\n 5. Introducing MANtIS: a novel Multi-Domain Information Seeking Dialogues Dataset\\n Summary: Wrote by Gustavo Penha et al.: This paper presents a novel conceptual model for conversational search defined in terms of conversational goals. The authors elicit how existing tasks and test collections from the fields of information retrieval (IR), natural language processing (NLP), and dialogue systems (DS) fit into this model. They describe a set of characteristics that an ideal conversational search dataset should have. Lastly, they introduce MANtIS, a large-scale dataset containing multi-domain and grounded information-seeking dialogues that fulfill all their dataset desiderata. They provide baseline results for conversation response ranking and user intent prediction tasks\\n Limitations: Limited dialogue complexity. The dataset contains only two-turn dialogues, which may not capture the complexity of real-world information-seeking dialogues with multiple turns.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: write a paragraph of literature review of the topic: information-seeking dialogues for my thesis. \n",
      "\n",
      " Instructions:\n",
      " 1 please use author names instead of paper names in the literature review \n",
      " 2 you should state the limitations of each paper \n",
      " 3 use the following papers and corresponding summary and limitations \n",
      " paper 1. Structuring collaborative information-seeking dialogues \n",
      " Summary: Wrote by Adelheit Stein and Elisabeth Maier: This paper presents a conversational interaction model that can be applied to graphical and multimodal interactions. The model comprises two interrelated parts: first, the description of local discourse structures and functional interrelations between dialogue acts to capture (local) conversational tactics, and second, the description of global structures by so-called dialogue scripts which are related to (global) information-seeking strategies.\n",
      " Limitation: Limited applicability. The paper presents a specific framework for structuring collaborative information-seeking dialogues that may not be applicable to all scenarios or contexts.\n",
      " paper 2. A Constraint-Based Approach for Cooperative Information-Seeking Dialogue\n",
      " Summary: Wrote by Yan Qu and Nancy Green: This paper presents a dialogue generation model that uses a Constraint-Based Problem-Solver (CBPS) to support cooperative mixed-initiative information-seeking dialogue. Use of the CBPS enables a dialogue system to 1) incrementally interleave query construction with solution construction 2) immediately detect under-constrained and overconstrained information requests, and 3) provide cooperative responses when these types of problems are detected\n",
      " Limitations: Limited scalability. The approach is based on a set of pre-defined constraints, which may not scale well to large or complex information-seeking tasks.\n",
      " paper 3. Modeling the illocutionary aspects of information-seeking dialogues\n",
      " Summary: This paper presents a dialogue model that incorporates the illocutionary aspects of information-seeking dialogues. Directive, commissive, and assertive types of dialogue acts (e.g., asking, offering, rejecting, answering, and evaluating) are represented by a complex transition network. The model determines all legitimate types and sequences of dialogue acts and regulates role assignments (e.g., when the information seeker and the information provider temporarily exchange their roles). Finally, an approach to integrating the illocutionary layer with other layers—which deal with thematical and rhetorical coherence—is outlined\n",
      " Limitations: Limited scope. The paper focuses only on modeling illocutionary acts in information-seeking dialogues, without addressing other important aspects such as task coordination and feedback.\n",
      " paper 4. QRFA: A Data-Driven Model of Information-Seeking Dialogues\n",
      " Summary: This paper presents a new model for information-seeking dialogues called QRFA (Query, Request, Feedback, Answer). The authors apply process mining techniques to discover patterns in conversational transcripts across multiple conversational datasets from different domains. They show that their QRFA model better reflects conversation flows observed in real information-seeking conversations than models proposed previously. Moreover, QRFA allows them to identify malfunctioning in dialogue system transcripts as deviations from the expected conversation flow described by their model via conformance analysis\n",
      " Limitations: Dependency on annotated data. The proposed model relies on annotated data for training and evaluation, which may be difficult and expensive to obtain for certain types of information-seeking dialogues.\n",
      " paper 5. Introducing MANtIS: a novel Multi-Domain Information Seeking Dialogues Dataset\n",
      " Summary: Wrote by Gustavo Penha et al.: This paper presents a novel conceptual model for conversational search defined in terms of conversational goals. The authors elicit how existing tasks and test collections from the fields of information retrieval (IR), natural language processing (NLP), and dialogue systems (DS) fit into this model. They describe a set of characteristics that an ideal conversational search dataset should have. Lastly, they introduce MANtIS, a large-scale dataset containing multi-domain and grounded information-seeking dialogues that fulfill all their dataset desiderata. They provide baseline results for conversation response ranking and user intent prediction tasks\n",
      " Limitations: Limited dialogue complexity. The dataset contains only two-turn dialogues, which may not capture the complexity of real-world information-seeking dialogues with multiple turns.\n",
      "\n",
      "\n",
      "Information-seeking dialogues have been studied extensively in the field of natural language processing (NLP). From a conversational interaction model proposed by Adelheit Stein and Elisabeth Maier to the idea of a multi-domain dataset by Gustavo Penha et al., various approaches have been presented to structure, generate, and model information-seeking dialogues. However, each of these papers has its own limitations. Stein and Maier's model, while providing valuable insights, may have limited applicability. Yan Qu and Nancy Green's dialogue generation model may not scale well in larger or more complex scenarios. The dialogue model presented by Catherine Lai et al. only covers illocutionary aspects of information-seeking dialogues and neglects other important aspects such as task coordination and feedback. The data-driven model proposed by Yashar Mehdad et al. relies heavily on annotated data for training and evaluation, which can be challenging to obtain. Finally, while the multi-domain dataset introduced by Gustavo Penha et al. is adequate for benchmarking, it contains only two-turn dialogues, which may not capture the complexity of real-world information-seeking dialogues that require multiple turns.\n"
     ]
    }
   ],
   "source": [
    "result = generate_response0(prompt,'gpt-3.5-turbo')\n",
    "input_string = result['choices'][0]['message']['content']\n",
    "print(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write the following text into an academic way: Raviteja Anantha et al. introduced a new dataset for Question Rewriting in Conversational Context (QReCC). They use question rewriting, answer retrieval, answer extraction as pipeline to build the dataset according to previous question dataset and webpages. The QReCC dataset provides annotations that allow people to train and evaluate individual subtasks of question rewriting. However, this dataset is Limited to simple questions, it cannot answer complex or multi-faceted questions, which require a more nuanced understanding of the topic.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Task: add a sentence to a given paragraph to speak about data synthesis using large language models in general and then go on to say that we want to use this progress for tutoring data, which no one has done before. \\n \\nInstruduction:\\n1. Write in an academic way\\n2. leave the \\cite{} part unchanged\\n3. the given paragraph is: Dialogue tutoring systems have shown great potential in enhancing learning outcomes in a variety of domains, such as math, science, and language learning\\cite{graesser2001intelligent}. However, the development of such systems is hindered by the scarcity of high-quality dialogue tutoring datasets that can support the training of effective dialogue models. Existing datasets are often plagued by low tutoring quality, small data sizes, or overly complex settings, such as classroom transcripts\\cite{cullen2001use}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: add a sentence to a given paragraph to speak about data synthesis using large language models in general and then go on to say that we want to use this progress for tutoring data, which no one has done before. \n",
      " \n",
      "Instruduction:\n",
      "1. Write in an academic way\n",
      "2. leave the \\cite{} part unchanged\n",
      "3. the given paragraph is: Dialogue tutoring systems have shown great potential in enhancing learning outcomes in a variety of domains, such as math, science, and language learning\\cite{graesser2001intelligent}. However, the development of such systems is hindered by the scarcity of high-quality dialogue tutoring datasets that can support the training of effective dialogue models. Existing datasets are often plagued by low tutoring quality, small data sizes, or overly complex settings, such as classroom transcripts\\cite{cullen2001use}.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3 question and answer, each of them are corelated between each other\n",
    "prompt = \"Task: given a paragraph, ask 3 questions based on the paragraph and give one answer for each question\\nInstruction:\\n1. The three questions should be in a progressive relationship, with each question going deeper than the previous one\\n2. The answer to each question should use the original sentence in the paragraph and cannot be modified\\n2. the given paragraph is:\\nThe measurements in the paper example are both accurate and precise, but in some cases, measurements are accurate but not precise, or they are precise but not accurate. Let us consider an example of a GPS system that is attempting to locate the position of a restaurant in a city. Think of the restaurant location as existing at the center of a bull’s-eye target, and think of each GPS attempt to locate the restaurant as a black dot. In Figure 1.24, you can see that the GPS measurements are spread out far apart from each other, but they are all relatively close to the actual location of the restaurant at the center of the target. This indicates a low precision, high accuracy measuring system. However, in Figure 1.25, the GPS measurements are concentrated quite closely to one another, but they are far away from the target location. This indicates a high precision, low accuracy measuring system.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: given a paragraph, ask 3 questions based on the paragraph and give one answer for each question\n",
      "Instruction:\n",
      "1. The three questions should be in a progressive relationship, with each question going deeper than the previous one\n",
      "2. The answer to each question should use the original sentence in the paragraph and cannot be modified\n",
      "2. the given paragraph is:\n",
      "The measurements in the paper example are both accurate and precise, but in some cases, measurements are accurate but not precise, or they are precise but not accurate. Let us consider an example of a GPS system that is attempting to locate the position of a restaurant in a city. Think of the restaurant location as existing at the center of a bull’s-eye target, and think of each GPS attempt to locate the restaurant as a black dot. In Figure 1.24, you can see that the GPS measurements are spread out far apart from each other, but they are all relatively close to the actual location of the restaurant at the center of the target. This indicates a low precision, high accuracy measuring system. However, in Figure 1.25, the GPS measurements are concentrated quite closely to one another, but they are far away from the target location. This indicates a high precision, low accuracy measuring system.\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read from txt file\n",
    "with open('C:/Users/Junling_W/Desktop/temp_gpt4.txt', 'r') as file:\n",
    "    prompt = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 3 question and answer, each of them are corelated between each other\n",
    "prompt = \"Task: given a paragraph, ask 3 questions based on the paragraph and give one answer for each question\\nInstruction:\\n1. The three questions should be in a progressive relationship, with each question going deeper than the previous one\\n2. The answer to each question should use the original sentence in the paragraph and cannot be modified\\n2. the given paragraph is:\\nThe measurements in the paper example are both accurate and precise, but in some cases, measurements are accurate but not precise, or they are precise but not accurate. Let us consider an example of a GPS system that is attempting to locate the position of a restaurant in a city. Think of the restaurant location as existing at the center of a bull’s-eye target, and think of each GPS attempt to locate the restaurant as a black dot. In Figure 1.24, you can see that the GPS measurements are spread out far apart from each other, but they are all relatively close to the actual location of the restaurant at the center of the target. This indicates a low precision, high accuracy measuring system. However, in Figure 1.25, the GPS measurements are concentrated quite closely to one another, but they are far away from the target location. This indicates a high precision, low accuracy measuring system.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task: given the program of trading bitcoin, change the sell strategy as: if I have 3 sells within the last 5 minutes, I will sell all my ETH at once\n",
      "Instructions:\n",
      "1. the first column of trade_history is the timestamp of this trade, use this timestamp to identify is there 3 sells in the last 5 minutes\n",
      "2. try to understand the logic of the program and debug it if necessary\n",
      "3. if there is any thing you can do to increase my profit, do it\n",
      "4. the given program: \n",
      "import random\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime\n",
      "\n",
      "# Function to run the program with specified parameters\n",
      "def run_program(rising_period, buy_amount, stop_loss, take_profit, trailing_stop_loss,sell_percentage,data):\n",
      "\n",
      "    data = data\n",
      "    # Add moving averages\n",
      "    data['MA_short'] = data['close'].rolling(window=10).mean()\n",
      "    data['MA_long'] = data['close'].rolling(window=50).mean()\n",
      "\n",
      "    # Define trading parameters\n",
      "    rising_period = rising_period\n",
      "    # buy_percentage = 0.006\n",
      "    buy_amount = buy_amount\n",
      "    stop_loss = stop_loss\n",
      "    take_profit = take_profit\n",
      "    trailing_stop_loss = trailing_stop_loss\n",
      "\n",
      "    # Initialize variables\n",
      "    usdt = 100\n",
      "    eth = 0\n",
      "    buy_price = 0\n",
      "    highest_price = 0\n",
      "    trade_history = []\n",
      "\n",
      "    # Iterate through data\n",
      "    for i in range(max(rising_period, 50), len(data)):\n",
      "        current_price = data['close'][i]\n",
      "        short_ma = data['MA_short'][i]\n",
      "        long_ma = data['MA_long'][i]\n",
      "\n",
      "        # Buy condition\n",
      "        if short_ma > long_ma and data['close'][i-rising_period:i].is_monotonic_increasing:\n",
      "            if usdt > 0:\n",
      "                buy_price = current_price\n",
      "                eth += buy_amount\n",
      "                usdt -= buy_amount * buy_price\n",
      "                trade_history.append((data.index[i], 'buy', buy_price, eth))\n",
      "\n",
      "        # Sell conditions\n",
      "        if eth > 0:\n",
      "            if current_price >= buy_price * take_profit or current_price <= buy_price * stop_loss:\n",
      "                usdt += eth * current_price * 0.995 * sell_percentage\n",
      "                trade_history.append((data.index[i], 'sell', current_price, eth))\n",
      "                eth = eth * (1 - sell_percentage)\n",
      "\n",
      "            # Trailing stop loss\n",
      "            if current_price > highest_price:\n",
      "                highest_price = current_price\n",
      "            elif current_price <= highest_price * (1 - trailing_stop_loss):\n",
      "                usdt += eth * current_price * 0.995 * sell_percentage\n",
      "                trade_history.append((data.index[i], 'sell', current_price, eth))\n",
      "                eth = eth * (1 - sell_percentage)\n",
      "\n",
      "    # Print trade history and profit\n",
      "    print('Trade history:')\n",
      "    for trade in trade_history:\n",
      "        print(trade[0], trade[1], trade[2], trade[3])\n",
      "    #save trade history to txt file\n",
      "    with open('trade_history.txt', 'w') as f:\n",
      "        for trade in trade_history:\n",
      "            f.write(str(trade[0]) + ' ' + str(trade[1]) + ' ' + str(trade[2]) + ' ' + str(trade[3]) + '\\n')\n",
      "    \n",
      "    total_balance = usdt + eth * data['close'][-1]\n",
      "    profit = total_balance - 100 # profit in USDT\n",
      "    print('\\nProfit:', profit)\n",
      "    return profit  # return the profit at the end of the program\n",
      "\n",
      "# # Number of iterations for random search\n",
      "# n_iterations = 1000\n",
      "\n",
      "# Store the results\n",
      "results = []\n",
      "data = pd.read_pickle('ETHUSDT_data.pkl')\n",
      "# Random search\n",
      "# for i in range(n_iterations):\n",
      "    # print(\"i = \",i)\n",
      "    # Generate random parameter values\n",
      "rp = 18\n",
      "ba = 0.006\n",
      "sl = 0.09877\n",
      "tp = 1.0214\n",
      "tsl = 0.043118740184396355\n",
      "sp = 0.7166\n",
      "\n",
      "# Run the program with the generated parameters\n",
      "profit = run_program(rp, ba, sl, tp, tsl,sp,data)\n",
      "print('profit = ',profit)\n",
      "***********************************************************************\n",
      "To modify the given program according to your request, we need to change the sell strategy with the following steps:\n",
      "\n",
      "1. Keep track of the last 5 minutes' sell actions\n",
      "2. If there are 3 or more sell actions within the last 5 minutes, sell all remaining ETH at once\n",
      "\n",
      "Here's the modified version of the program:\n",
      "\n",
      "```python\n",
      "import random\n",
      "import requests\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "from datetime import datetime, timedelta\n",
      "\n",
      "# Function to run the program with specified parameters\n",
      "def run_program(rising_period, buy_amount, stop_loss, take_profit, trailing_stop_loss, sell_percentage, data):\n",
      "\n",
      "    data = data\n",
      "    # Add moving averages\n",
      "    data['MA_short'] = data['close'].rolling(window=10).mean()\n",
      "    data['MA_long'] = data['close'].rolling(window=50).mean()\n",
      "\n",
      "    # Define trading parameters\n",
      "    rising_period = rising_period\n",
      "    buy_amount = buy_amount\n",
      "    stop_loss = stop_loss\n",
      "    take_profit = take_profit\n",
      "    trailing_stop_loss = trailing_stop_loss\n",
      "\n",
      "    # Initialize variables\n",
      "    usdt = 100\n",
      "    eth = 0\n",
      "    buy_price = 0\n",
      "    highest_price = 0\n",
      "    trade_history = []\n",
      "\n",
      "    # Sell all ETH in one shot if having 3 sell actions within the last 5 minutes\n",
      "    def sell_all_eth(current_price, eth, usdt):\n",
      "        usdt += eth * current_price * 0.995\n",
      "        trade_history.append((data.index[i], 'sell all', current_price, eth))\n",
      "        eth = 0\n",
      "        return eth, usdt\n",
      "\n",
      "    # Iterate through data\n",
      "    for i in range(max(rising_period, 50), len(data)):\n",
      "        current_price = data['close'][i]\n",
      "        short_ma = data['MA_short'][i]\n",
      "        long_ma = data['MA_long'][i]\n",
      "\n",
      "        # Buy condition\n",
      "        if short_ma > long_ma and data['close'][i-rising_period:i].is_monotonic_increasing:\n",
      "            if usdt > 0:\n",
      "                buy_price = current_price\n",
      "                eth += buy_amount\n",
      "                usdt -= buy_amount * buy_price\n",
      "                trade_history.append((data.index[i], 'buy', buy_price, eth))\n",
      "\n",
      "        # Sell conditions\n",
      "        if eth > 0:\n",
      "            # Check for 3 sell actions within the last 5 minutes\n",
      "            last_5_minutes_sell_count = 0\n",
      "            for trade in trade_history[::-1]:\n",
      "                if trade[0] > (data.index[i] - timedelta(minutes=5)):\n",
      "                    if trade[1] == 'sell':\n",
      "                        last_5_minutes_sell_count += 1\n",
      "                else:\n",
      "                    break\n",
      "\n",
      "            if last_5_minutes_sell_count >= 3:\n",
      "                eth, usdt = sell_all_eth(current_price, eth, usdt)\n",
      "            else:\n",
      "                if current_price >= buy_price * take_profit or current_price <= buy_price * stop_loss:\n",
      "                    usdt += eth * current_price * 0.995 * sell_percentage\n",
      "                    trade_history.append((data.index[i], 'sell', current_price, eth * sell_percentage))\n",
      "                    eth = eth * (1 - sell_percentage)\n",
      "\n",
      "                # Trailing stop loss\n",
      "                if current_price > highest_price:\n",
      "                    highest_price = current_price\n",
      "                elif current_price <= highest_price * (1 - trailing_stop_loss):\n",
      "                    usdt += eth * current_price * 0.995 * sell_percentage\n",
      "                    trade_history.append((data.index[i], 'sell', current_price, eth * sell_percentage))\n",
      "                    eth = eth * (1 - sell_percentage)\n",
      "\n",
      "    # Print trade history and profit\n",
      "    print('Trade history:')\n",
      "    for trade in trade_history:\n",
      "        print(trade[0], trade[1], trade[2], trade[3])\n",
      "    #save trade history to txt file\n",
      "    with open('trade_history.txt', 'w') as f:\n",
      "        for trade in trade_history:\n",
      "            f.write(str(trade[0]) + ' ' + str(trade[1]) + ' ' + str(trade[2]) + ' ' + str(trade[3]) + '\\n')\n",
      "    \n",
      "    total_balance = usdt + eth * data['close'][-1]\n",
      "    profit = total_balance - 100 # profit in USDT\n",
      "    print('\\nProfit:', profit)\n",
      "    return profit  # return the profit at the end of the program\n",
      "\n",
      "# Store the results\n",
      "results = []\n",
      "data = pd.read_pickle('ETHUSDT_data.pkl')\n",
      "\n",
      "rp = 18\n",
      "ba = 0.006\n",
      "sl = 0.09877\n",
      "tp = 1.0214\n",
      "tsl = 0.043118740184396355\n",
      "sp = 0.7166\n",
      "\n",
      "# Run the program with the generated parameters\n",
      "profit = run_program(rp, ba, sl, tp, tsl, sp, data)\n",
      "print('profit = ', profit)\n",
      "```\n",
      "\n",
      "Please note that this sell strategy might not necessarily increase the profit. Keep testing and refining your strategy to optimize the profit.\n"
     ]
    }
   ],
   "source": [
    "result = generate_response0(prompt,'gpt-4-0314')\n",
    "input_string = result['choices'][0]['message']['content']\n",
    "print('***********************************************************************')\n",
    "print(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject chat.completion id=chatcmpl-6xFShDhKGjRdqFbdjgOBBd9zMOArR at 0x208ebccdf48> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"stop\",\n",
       "      \"index\": 0,\n",
       "      \"message\": {\n",
       "        \"content\": \"In the quest to optimize information-seeking dialogues, several studies have proposed various models to address different aspects of these exchanges. Stein and Maier (paper 1) present a conversational interaction model focused on local and global discourse structures. However, the limited applicability of their framework restricts it from being implemented in all scenarios or contexts. Qu and Green (paper 2) propose a dialogue generation model based on a Constraint-Based Problem-Solver (CBPS), which enables the dialogue system to support cooperative mixed-initiative information-seeking dialogues. The primary limitation of this approach lies in its limited scalability due to the reliance on pre-defined constraints. Another approach to modeling information-seeking dialogues comes from the paper that focuses on incorporating illocutionary acts (paper 3). Despite its specialization in modeling illocutionary aspects, it does not address other crucial factors like task coordination and feedback.\\n\\nIn contrast, the QRFA model (paper 4) better reflects conversation flows observed in real information-seeking conversations by employing process mining techniques on conversational transcripts across domains. Nevertheless, the model's dependency on annotated data for training and evaluation presents challenges in obtaining suitable data for specific information-seeking dialogues. Lastly, Penha et al. (paper 5) introduce the MANtIS dataset, which is grounded on a novel conceptual model for conversational search, defined in terms of conversational goals. Although the dataset contains multi-domain and grounded information-seeking dialogues, the limitation of having only two-turn dialogues hinders the dataset's ability to capture the complexity of real-world information-seeking dialogues with multiple turns. In summary, each study provides valuable insights into different aspects of information-seeking dialogues, but their limitations highlight the challenges in developing comprehensive and scalable models.\",\n",
       "        \"role\": \"assistant\"\n",
       "      }\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1679578807,\n",
       "  \"id\": \"chatcmpl-6xFShDhKGjRdqFbdjgOBBd9zMOArR\",\n",
       "  \"model\": \"gpt-4-0314\",\n",
       "  \"object\": \"chat.completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 353,\n",
       "    \"prompt_tokens\": 846,\n",
       "    \"total_tokens\": 1199\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the quest to optimize information-seeking dialogues, several studies have proposed various models to address different aspects of these exchanges. Stein and Maier (paper 1) present a conversational interaction model focused on local and global discourse structures. However, the limited applicability of their framework restricts it from being implemented in all scenarios or contexts. Qu and Green (paper 2) propose a dialogue generation model based on a Constraint-Based Problem-Solver (CBPS), which enables the dialogue system to support cooperative mixed-initiative information-seeking dialogues. The primary limitation of this approach lies in its limited scalability due to the reliance on pre-defined constraints. Another approach to modeling information-seeking dialogues comes from the paper that focuses on incorporating illocutionary acts (paper 3). Despite its specialization in modeling illocutionary aspects, it does not address other crucial factors like task coordination and feedback.\n",
      "\n",
      "In contrast, the QRFA model (paper 4) better reflects conversation flows observed in real information-seeking conversations by employing process mining techniques on conversational transcripts across domains. Nevertheless, the model's dependency on annotated data for training and evaluation presents challenges in obtaining suitable data for specific information-seeking dialogues. Lastly, Penha et al. (paper 5) introduce the MANtIS dataset, which is grounded on a novel conceptual model for conversational search, defined in terms of conversational goals. Although the dataset contains multi-domain and grounded information-seeking dialogues, the limitation of having only two-turn dialogues hinders the dataset's ability to capture the complexity of real-world information-seeking dialogues with multiple turns. In summary, each study provides valuable insights into different aspects of information-seeking dialogues, but their limitations highlight the challenges in developing comprehensive and scalable models.\n"
     ]
    }
   ],
   "source": [
    "input_string = result['choices'][0]['message']['content']\n",
    "print(input_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
